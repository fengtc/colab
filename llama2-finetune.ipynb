{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c3ef31",
   "metadata": {},
   "source": [
    "## Fine-tuning Llama 2 with custom data\n",
    "In this guide, I show you how easy it is to leverage Huggingface libraries to finetune Llama 2 with your own dataset. All you need is to put together a json-lines file of your dataset. Huggingface's new trl library will then handle the rest!!\n",
    "\n",
    "All you need is your data in this format:\n",
    "```\n",
    "{\"text\": \"text-for-model-to-predict\"}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Setup:\n",
    "To start, use a machine with a reasonably latest version of python and cuda. (I used Python 3.10 and cuda 11.7). On the GPU side, I recommend at least 24GB of VRam: A100, A10, A10G etc. If you want to fine-tune the larger 13b and 70b models, I'd go straight for the A100 :)\n",
    "\n",
    "First step is always to install those pesky dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd63725-1dd9-46dd-840a-10a0a90b6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub\n",
    "!pip install -q -U trl transformers accelerate peft\n",
    "!pip install -q -U datasets bitsandbytes einops wandb\n",
    "!pip install  -q ipywidgets\n",
    "!pip install -q scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b15ea",
   "metadata": {},
   "source": [
    "And import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec8b54-af50-4b60-94d2-51c92d9f1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments \n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d7643",
   "metadata": {},
   "source": [
    "## Preparing and loading your dataset\n",
    "Make sure you store your dataset in the following format:\n",
    "```\n",
    "{\"text\": \"text-for-model-to-predict\"}\n",
    "```\n",
    "(The \"text\" feature is extracted later by the training code.)\n",
    "\n",
    "\n",
    "With Large Language Models, you tend to want to format your training samples in a way that gives the model the most possible information about the task it is trying to do. And the best way to reformat your data into a .jsonl file is to ask ChatGPT of course. I extracted my dataset from my Google Keep notes. Each note has a Title, some Labels and the Content. Each sample I generated then looked like this:\n",
    "\n",
    "```\n",
    "{\"text\": \n",
    "\"A note has the following\n",
    "Title: \n",
    "Labels: \n",
    "Content: \"\n",
    "}\n",
    "```\n",
    "\n",
    "These are the first three lines of my .jsonl file:\n",
    "```\n",
    "{\"text\": \"A note has the following\\nTitle: Ip address \\nLabels: code \\nContent: Client ip address: 0.0.0.0\"}\n",
    "{\"text\": \"A note has the following\\nTitle: ICG application\\nLabels: \\nContent: Order of preference:\\nNkt\\n\"}\n",
    "{\"text\": \"A note has the following\\nTitle: Job ideas \\nLabels: \\nContent: Joining a big tech company could be a good idea\"}\n",
    "```\n",
    "\n",
    "I used a dataset that I generated myself of 4079 notes from my Google Keep. If you want to download a dataset from the Huggingface Hub, just pass in the name of the dataset like this: ```load_dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d650f-e6cd-46e1-bf36-d5851af8bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='notes.jsonl', split='train') # \n",
    "eval_dataset = load_dataset('json', data_files='notes_validation.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6e4b7",
   "metadata": {},
   "source": [
    "(If you don't want to use an evaluation dataset, just comment out the above eval_dataset line and the other appearences of it in below in SFTTrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e38b3",
   "metadata": {},
   "source": [
    "The we load the Llama 2 non-chat model quantized to 4 bits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67849376-1ca2-4ceb-9757-74b31e507e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1 \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c2717",
   "metadata": {},
   "source": [
    "And setup a train so that we log, save and evaluate every 50 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa1e98-34ee-44ba-878d-59c35b3a4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./Llama-2-7b-hf-fine-tune-baby\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    max_steps=1000,\n",
    "    logging_dir=\"./logs\",        # Directory for storing logs\n",
    "    save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "    save_steps=50,                # Save checkpoints every 50 steps\n",
    "    evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "    eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "    do_eval=True                 # Perform evaluation at the end of training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f5edc",
   "metadata": {},
   "source": [
    "We set the config for the Lora adapter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d435a1",
   "metadata": {},
   "source": [
    "(I experimented with higher alpha and r - and found poorer results...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a86267",
   "metadata": {},
   "source": [
    "Then leverage the beautiful SFTTrainer class from Huggingface to fine tune Llama 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ab6ee-01ff-4ba6-83b3-56342eb5041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  \n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "\n",
    "# pass in resume_from_checkpoint=True to resume from a checkpoint\n",
    "trainer.train() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69adefc4",
   "metadata": {},
   "source": [
    "On a 40GB A100, this took about 2 hours ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2eb12",
   "metadata": {},
   "source": [
    "## Running inference on a trained model\n",
    "By default, the PEFT library will only save the Qlora adapters. So we need to load the base Llama 2 model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name=\"meta-llama/Llama-2-7b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b0caf6",
   "metadata": {},
   "source": [
    "and load the qlora adapter from a checkpoint directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93beb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, \"/root/llama2sfft-testing/Llama-2-7b-hf-qlora-full-dataset/checkpoint-900\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e17d1c",
   "metadata": {},
   "source": [
    "then run some inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"A note has the following\\nTitle: \\nLabels: \\nContent: i love\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
