{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fengtc/colab/blob/master/llama2_apply_download_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIRwPkAIZ5-m"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/llama.git\n",
        "!cd /content/llama && bash download.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtnrYTQmQ78b"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install accelerate\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXX8YiJ9Sshe"
      },
      "outputs": [],
      "source": [
        "!python convert_llama_weights_to_hf.py \\\n",
        "    --input_dir /content/llama/llama-2-7b  --model_size 7B --output_dir /content/llama/models_hf/7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxsIH838UT09"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/llama-recipes.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI5EsA6K5ZaY"
      },
      "outputs": [],
      "source": [
        "cd llama-recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ9dgPw_5YxQ"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mocwFdw2cdZZ"
      },
      "outputs": [],
      "source": [
        "#测试没有微调之前的模型\n",
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "model_id=\"/content/llama/models_hf/7B\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98L9uQ5CdJkO"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Summarize this dialog:\n",
        "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
        "B: I’m pretty sure I am. What’s up?\n",
        "A: Can you go with me to the animal shelter?.\n",
        "B: What do you want to do?\n",
        "A: I want to get a puppy for my son.\n",
        "B: That will make him so happy.\n",
        "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
        "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
        "A: I'll get him one of those little dogs.\n",
        "B: One that won't grow up too big;-)\n",
        "A: And eat too much;-))\n",
        "B: Do you know which one he would like?\n",
        "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
        "B: I bet you had to drag him away.\n",
        "A: He wanted to take it home right away ;-).\n",
        "B: I wonder what he'll name it.\n",
        "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
        "---\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BdAiHEEDWCP"
      },
      "outputs": [],
      "source": [
        "#解决colab字符集错误\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEHsH-HfVKnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febd42a5-afb3-41e0-ea8c-d9e11283603e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-zku9g71ijkf3 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "2023-08-03 03:14:54.158431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading checkpoint shards: 100% 2/2 [00:11<00:00,  5.72s/it]\n",
            "--> Model /content/llama/models_hf/7B\n",
            "\n",
            "--> /content/llama/models_hf/7B has 262.41024 Million params\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:119: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565, and set the legacy attribute accordingly.\n",
            "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
            "Downloading builder script: 100% 3.36k/3.36k [00:00<00:00, 22.6MB/s]\n",
            "Downloading metadata: 100% 1.58k/1.58k [00:00<00:00, 13.2MB/s]\n",
            "Downloading readme: 100% 7.04k/7.04k [00:00<00:00, 30.7MB/s]\n",
            "Downloading data: 100% 2.94M/2.94M [00:00<00:00, 96.6MB/s]\n",
            "Generating train split: 100% 14732/14732 [00:00<00:00, 17402.04 examples/s]\n",
            "Generating test split: 100% 819/819 [00:00<00:00, 3117.13 examples/s]\n",
            "Generating validation split: 100% 818/818 [00:00<00:00, 3179.64 examples/s]\n",
            "Map: 100% 14732/14732 [00:00<00:00, 20979.38 examples/s]\n",
            "Map: 100% 14732/14732 [00:14<00:00, 988.03 examples/s]\n",
            "Map: 100% 14732/14732 [00:03<00:00, 4261.67 examples/s]\n",
            "--> Training Set Length = 1555\n",
            "Map: 100% 818/818 [00:00<00:00, 19499.85 examples/s]\n",
            "Map: 100% 818/818 [00:00<00:00, 1012.02 examples/s]\n",
            "Map: 100% 818/818 [00:00<00:00, 4340.83 examples/s]\n",
            "--> Validation Set Length = 84\n",
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "Training Epoch0:   0% 0/388 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "\n",
            " step 0 is completed and loss is 2.0127763748168945\n",
            "Training Epoch0:   0% 1/388 [00:12<1:23:22, 12.93s/it]\n",
            " step 1 is completed and loss is 2.010692596435547\n",
            "Training Epoch0:   1% 2/388 [00:25<1:20:10, 12.46s/it]\n",
            " step 2 is completed and loss is 1.99779212474823\n",
            "Training Epoch0:   1% 3/388 [00:37<1:19:13, 12.35s/it]\n",
            " step 3 is completed and loss is 2.0959179401397705\n",
            "Training Epoch0:   1% 4/388 [00:49<1:18:28, 12.26s/it]\n",
            " step 4 is completed and loss is 1.9045512676239014\n",
            "Training Epoch0:   1% 5/388 [01:01<1:17:58, 12.22s/it]\n",
            " step 5 is completed and loss is 1.9557145833969116\n",
            "Training Epoch0:   2% 6/388 [01:13<1:17:36, 12.19s/it]\n",
            " step 6 is completed and loss is 1.919350504875183\n",
            "Training Epoch0:   2% 7/388 [01:25<1:17:18, 12.17s/it]\n",
            " step 7 is completed and loss is 1.824275016784668\n",
            "Training Epoch0:   2% 8/388 [01:37<1:17:02, 12.16s/it]\n",
            " step 8 is completed and loss is 1.92013680934906\n",
            "Training Epoch0:   2% 9/388 [01:50<1:16:48, 12.16s/it]\n",
            " step 9 is completed and loss is 1.865333080291748\n",
            "Training Epoch0:   3% 10/388 [02:02<1:16:33, 12.15s/it]\n",
            " step 10 is completed and loss is 1.8082844018936157\n",
            "Training Epoch0:   3% 11/388 [02:14<1:16:20, 12.15s/it]\n",
            " step 11 is completed and loss is 1.7915573120117188\n",
            "Training Epoch0:   3% 12/388 [02:26<1:16:07, 12.15s/it]\n",
            " step 12 is completed and loss is 1.8153544664382935\n",
            "Training Epoch0:   3% 13/388 [02:38<1:16:00, 12.16s/it]\n",
            " step 13 is completed and loss is 1.7067337036132812\n",
            "Training Epoch0:   4% 14/388 [02:50<1:15:47, 12.16s/it]\n",
            " step 14 is completed and loss is 1.7843067646026611\n",
            "Training Epoch0:   4% 15/388 [03:03<1:15:36, 12.16s/it]\n",
            " step 15 is completed and loss is 1.8325037956237793\n",
            "Training Epoch0:   4% 16/388 [03:15<1:15:22, 12.16s/it]\n",
            " step 16 is completed and loss is 1.8476569652557373\n",
            "Training Epoch0:   4% 17/388 [03:27<1:15:08, 12.15s/it]\n",
            " step 17 is completed and loss is 1.8197002410888672\n",
            "Training Epoch0:   5% 18/388 [03:39<1:14:56, 12.15s/it]\n",
            " step 18 is completed and loss is 1.7839733362197876\n",
            "Training Epoch0:   5% 19/388 [03:51<1:14:43, 12.15s/it]\n",
            " step 19 is completed and loss is 1.751089334487915\n",
            "Training Epoch0:   5% 20/388 [04:03<1:14:31, 12.15s/it]\n",
            " step 20 is completed and loss is 1.8714640140533447\n",
            "Training Epoch0:   5% 21/388 [04:15<1:14:18, 12.15s/it]\n",
            " step 21 is completed and loss is 1.8175671100616455\n",
            "Training Epoch0:   6% 22/388 [04:28<1:14:07, 12.15s/it]\n",
            " step 22 is completed and loss is 1.8018053770065308\n",
            "Training Epoch0:   6% 23/388 [04:40<1:13:54, 12.15s/it]\n",
            " step 23 is completed and loss is 1.795673131942749\n",
            "Training Epoch0:   6% 24/388 [04:52<1:13:44, 12.15s/it]\n",
            " step 24 is completed and loss is 1.8229875564575195\n",
            "Training Epoch0:   6% 25/388 [05:04<1:13:31, 12.15s/it]\n",
            " step 25 is completed and loss is 1.7896533012390137\n",
            "Training Epoch0:   7% 26/388 [05:16<1:13:18, 12.15s/it]\n",
            " step 26 is completed and loss is 1.7993836402893066\n",
            "Training Epoch0:   7% 27/388 [05:28<1:13:05, 12.15s/it]\n",
            " step 27 is completed and loss is 1.902711033821106\n",
            "Training Epoch0:   7% 28/388 [05:41<1:12:58, 12.16s/it]\n",
            " step 28 is completed and loss is 1.7745561599731445\n",
            "Training Epoch0:   7% 29/388 [05:53<1:12:48, 12.17s/it]\n",
            " step 29 is completed and loss is 1.7185090780258179\n",
            "Training Epoch0:   8% 30/388 [06:05<1:12:37, 12.17s/it]\n",
            " step 30 is completed and loss is 1.819181203842163\n",
            "Training Epoch0:   8% 31/388 [06:17<1:12:22, 12.16s/it]\n",
            " step 31 is completed and loss is 1.765676736831665\n",
            "Training Epoch0:   8% 32/388 [06:29<1:12:09, 12.16s/it]\n",
            " step 32 is completed and loss is 1.8334307670593262\n",
            "Training Epoch0:   9% 33/388 [06:41<1:11:55, 12.16s/it]\n",
            " step 33 is completed and loss is 1.6906933784484863\n",
            "Training Epoch0:   9% 34/388 [06:54<1:11:43, 12.16s/it]\n",
            " step 34 is completed and loss is 1.8221707344055176\n",
            "Training Epoch0:   9% 35/388 [07:06<1:11:28, 12.15s/it]\n",
            " step 35 is completed and loss is 1.7621804475784302\n",
            "Training Epoch0:   9% 36/388 [07:18<1:11:16, 12.15s/it]\n",
            " step 36 is completed and loss is 1.8036375045776367\n",
            "Training Epoch0:  10% 37/388 [07:30<1:11:03, 12.15s/it]\n",
            " step 37 is completed and loss is 1.7823792695999146\n",
            "Training Epoch0:  10% 38/388 [07:42<1:10:53, 12.15s/it]\n",
            " step 38 is completed and loss is 1.7143129110336304\n",
            "Training Epoch0:  10% 39/388 [07:54<1:10:45, 12.17s/it]\n",
            " step 39 is completed and loss is 1.756718397140503\n",
            "Training Epoch0:  10% 40/388 [08:06<1:10:32, 12.16s/it]\n",
            " step 40 is completed and loss is 1.7394236326217651\n",
            "Training Epoch0:  11% 41/388 [08:19<1:10:22, 12.17s/it]\n",
            " step 41 is completed and loss is 1.7533961534500122\n",
            "Training Epoch0:  11% 42/388 [08:31<1:10:07, 12.16s/it]\n",
            " step 42 is completed and loss is 1.7116820812225342\n",
            "Training Epoch0:  11% 43/388 [08:43<1:09:53, 12.16s/it]\n",
            " step 43 is completed and loss is 1.725616693496704\n",
            "Training Epoch0:  11% 44/388 [08:55<1:09:42, 12.16s/it]\n",
            " step 44 is completed and loss is 1.6764440536499023\n",
            "Training Epoch0:  12% 45/388 [09:07<1:09:29, 12.15s/it]\n",
            " step 45 is completed and loss is 1.805329442024231\n",
            "Training Epoch0:  12% 46/388 [09:19<1:09:16, 12.15s/it]\n",
            " step 46 is completed and loss is 1.7754848003387451\n",
            "Training Epoch0:  12% 47/388 [09:32<1:09:03, 12.15s/it]\n",
            " step 47 is completed and loss is 1.7708039283752441\n",
            "Training Epoch0:  12% 48/388 [09:44<1:08:51, 12.15s/it]\n",
            " step 48 is completed and loss is 1.7208651304244995\n",
            "Training Epoch0:  13% 49/388 [09:56<1:08:40, 12.16s/it]\n",
            " step 49 is completed and loss is 1.7435543537139893\n",
            "Training Epoch0:  13% 50/388 [10:08<1:08:26, 12.15s/it]\n",
            " step 50 is completed and loss is 1.767507791519165\n",
            "Training Epoch0:  13% 51/388 [10:20<1:08:13, 12.15s/it]\n",
            " step 51 is completed and loss is 1.6659005880355835\n",
            "Training Epoch0:  13% 52/388 [10:32<1:08:01, 12.15s/it]\n",
            " step 52 is completed and loss is 1.7279152870178223\n",
            "Training Epoch0:  14% 53/388 [10:44<1:07:50, 12.15s/it]\n",
            " step 53 is completed and loss is 1.7335302829742432\n",
            "Training Epoch0:  14% 54/388 [10:57<1:07:37, 12.15s/it]\n",
            " step 54 is completed and loss is 1.7136616706848145\n",
            "Training Epoch0:  14% 55/388 [11:09<1:07:26, 12.15s/it]\n",
            " step 55 is completed and loss is 1.7176823616027832\n",
            "Training Epoch0:  14% 56/388 [11:21<1:07:13, 12.15s/it]\n",
            " step 56 is completed and loss is 1.658928394317627\n",
            "Training Epoch0:  15% 57/388 [11:33<1:07:06, 12.17s/it]\n",
            " step 57 is completed and loss is 1.6684757471084595\n",
            "Training Epoch0:  15% 58/388 [11:45<1:06:52, 12.16s/it]\n",
            " step 58 is completed and loss is 1.69996976852417\n",
            "Training Epoch0:  15% 59/388 [11:57<1:06:39, 12.16s/it]\n",
            " step 59 is completed and loss is 1.6942795515060425\n",
            "Training Epoch0:  15% 60/388 [12:10<1:06:32, 12.17s/it]\n",
            " step 60 is completed and loss is 1.6127429008483887\n",
            "Training Epoch0:  16% 61/388 [12:22<1:06:16, 12.16s/it]\n",
            " step 61 is completed and loss is 1.712162733078003\n",
            "Training Epoch0:  16% 62/388 [12:34<1:06:04, 12.16s/it]\n",
            " step 62 is completed and loss is 1.6281025409698486\n",
            "Training Epoch0:  16% 63/388 [12:46<1:05:59, 12.18s/it]\n",
            " step 63 is completed and loss is 1.644992709159851\n",
            "Training Epoch0:  16% 64/388 [12:58<1:05:43, 12.17s/it]\n",
            " step 64 is completed and loss is 1.6503337621688843\n",
            "Training Epoch0:  17% 65/388 [13:10<1:05:30, 12.17s/it]\n",
            " step 65 is completed and loss is 1.6383941173553467\n",
            "Training Epoch0:  17% 66/388 [13:23<1:05:16, 12.16s/it]\n",
            " step 66 is completed and loss is 1.7286821603775024\n",
            "Training Epoch0:  17% 67/388 [13:35<1:05:02, 12.16s/it]\n",
            " step 67 is completed and loss is 1.6246250867843628\n",
            "Training Epoch0:  18% 68/388 [13:47<1:04:57, 12.18s/it]\n",
            " step 68 is completed and loss is 1.7371920347213745\n",
            "Training Epoch0:  18% 69/388 [13:59<1:04:42, 12.17s/it]\n",
            " step 69 is completed and loss is 1.6774734258651733\n",
            "Training Epoch0:  18% 70/388 [14:11<1:04:27, 12.16s/it]\n",
            " step 70 is completed and loss is 1.6301088333129883\n",
            "Training Epoch0:  18% 71/388 [14:23<1:04:13, 12.16s/it]\n",
            " step 71 is completed and loss is 1.6740097999572754\n",
            "Training Epoch0:  19% 72/388 [14:36<1:04:01, 12.16s/it]\n",
            " step 72 is completed and loss is 1.6983697414398193\n",
            "Training Epoch0:  19% 73/388 [14:48<1:03:52, 12.17s/it]\n",
            " step 73 is completed and loss is 1.6572529077529907\n",
            "Training Epoch0:  19% 74/388 [15:00<1:03:38, 12.16s/it]\n",
            " step 74 is completed and loss is 1.612222671508789\n",
            "Training Epoch0:  19% 75/388 [15:12<1:03:26, 12.16s/it]\n",
            " step 75 is completed and loss is 1.747979760169983\n",
            "Training Epoch0:  20% 76/388 [15:24<1:03:13, 12.16s/it]\n",
            " step 76 is completed and loss is 1.6563963890075684\n",
            "Training Epoch0:  20% 77/388 [15:36<1:02:59, 12.15s/it]\n",
            " step 77 is completed and loss is 1.700644850730896\n",
            "Training Epoch0:  20% 78/388 [15:48<1:02:48, 12.16s/it]\n",
            " step 78 is completed and loss is 1.8244434595108032\n",
            "Training Epoch0:  20% 79/388 [16:01<1:02:35, 12.15s/it]\n",
            " step 79 is completed and loss is 1.7308648824691772\n",
            "Training Epoch0:  21% 80/388 [16:13<1:02:24, 12.16s/it]\n",
            " step 80 is completed and loss is 1.6975637674331665\n",
            "Training Epoch0:  21% 81/388 [16:25<1:02:11, 12.15s/it]\n",
            " step 81 is completed and loss is 1.674989938735962\n",
            "Training Epoch0:  21% 82/388 [16:37<1:01:58, 12.15s/it]\n",
            " step 82 is completed and loss is 1.7112077474594116\n",
            "Training Epoch0:  21% 83/388 [16:49<1:01:49, 12.16s/it]\n",
            " step 83 is completed and loss is 1.647651195526123\n",
            "Training Epoch0:  22% 84/388 [17:02<1:01:44, 12.19s/it]\n",
            " step 84 is completed and loss is 1.6911348104476929\n",
            "Training Epoch0:  22% 85/388 [17:14<1:01:28, 12.17s/it]\n",
            " step 85 is completed and loss is 1.6646602153778076\n",
            "Training Epoch0:  22% 86/388 [17:26<1:01:13, 12.16s/it]\n",
            " step 86 is completed and loss is 1.7422457933425903\n",
            "Training Epoch0:  22% 87/388 [17:38<1:00:59, 12.16s/it]\n",
            " step 87 is completed and loss is 1.6838133335113525\n",
            "Training Epoch0:  23% 88/388 [17:50<1:00:46, 12.16s/it]\n",
            " step 88 is completed and loss is 1.7333025932312012\n",
            "Training Epoch0:  23% 89/388 [18:02<1:00:33, 12.15s/it]\n",
            " step 89 is completed and loss is 1.6931296586990356\n",
            "Training Epoch0:  23% 90/388 [18:14<1:00:21, 12.15s/it]\n",
            " step 90 is completed and loss is 1.6349107027053833\n",
            "Training Epoch0:  23% 91/388 [18:27<1:00:12, 12.16s/it]\n",
            " step 91 is completed and loss is 1.7244956493377686\n",
            "Training Epoch0:  24% 92/388 [18:39<59:59, 12.16s/it]  \n",
            " step 92 is completed and loss is 1.7037110328674316\n",
            "Training Epoch0:  24% 93/388 [18:51<59:50, 12.17s/it]\n",
            " step 93 is completed and loss is 1.670130968093872\n",
            "Training Epoch0:  24% 94/388 [19:03<59:40, 12.18s/it]\n",
            " step 94 is completed and loss is 1.6390411853790283\n",
            "Training Epoch0:  24% 95/388 [19:15<59:24, 12.17s/it]\n",
            " step 95 is completed and loss is 1.6858056783676147\n",
            "Training Epoch0:  25% 96/388 [19:27<59:11, 12.16s/it]\n",
            " step 96 is completed and loss is 1.7238414287567139\n",
            "Training Epoch0:  25% 97/388 [19:40<58:57, 12.16s/it]\n",
            " step 97 is completed and loss is 1.8122448921203613\n",
            "Training Epoch0:  25% 98/388 [19:52<58:46, 12.16s/it]\n",
            " step 98 is completed and loss is 1.5764696598052979\n",
            "Training Epoch0:  26% 99/388 [20:04<58:32, 12.15s/it]\n",
            " step 99 is completed and loss is 1.7421804666519165\n",
            "Training Epoch0:  26% 100/388 [20:16<58:20, 12.15s/it]\n",
            " step 100 is completed and loss is 1.7070508003234863\n",
            "Training Epoch0:  26% 101/388 [20:28<58:07, 12.15s/it]\n",
            " step 101 is completed and loss is 1.6548492908477783\n",
            "Training Epoch0:  26% 102/388 [20:40<57:54, 12.15s/it]\n",
            " step 102 is completed and loss is 1.7752399444580078\n",
            "Training Epoch0:  27% 103/388 [20:52<57:41, 12.15s/it]\n",
            " step 103 is completed and loss is 1.691441535949707\n",
            "Training Epoch0:  27% 104/388 [21:05<57:29, 12.15s/it]\n",
            " step 104 is completed and loss is 1.61037015914917\n",
            "Training Epoch0:  27% 105/388 [21:17<57:17, 12.15s/it]\n",
            " step 105 is completed and loss is 1.6199275255203247\n",
            "Training Epoch0:  27% 106/388 [21:29<57:10, 12.16s/it]\n",
            " step 106 is completed and loss is 1.7107106447219849\n",
            "Training Epoch0:  28% 107/388 [21:41<56:59, 12.17s/it]\n",
            " step 107 is completed and loss is 1.696388602256775\n",
            "Training Epoch0:  28% 108/388 [21:53<56:51, 12.18s/it]\n",
            " step 108 is completed and loss is 1.7139250040054321\n",
            "Training Epoch0:  28% 109/388 [22:06<56:36, 12.17s/it]\n",
            " step 109 is completed and loss is 1.6852624416351318\n",
            "Training Epoch0:  28% 110/388 [22:18<56:22, 12.17s/it]\n",
            " step 110 is completed and loss is 1.7281640768051147\n",
            "Training Epoch0:  29% 111/388 [22:30<56:08, 12.16s/it]\n",
            " step 111 is completed and loss is 1.6559339761734009\n",
            "Training Epoch0:  29% 112/388 [22:42<55:55, 12.16s/it]\n",
            " step 112 is completed and loss is 1.7012271881103516\n",
            "Training Epoch0:  29% 113/388 [22:54<55:45, 12.17s/it]\n",
            " step 113 is completed and loss is 1.7654650211334229\n",
            "Training Epoch0:  29% 114/388 [23:06<55:31, 12.16s/it]\n",
            " step 114 is completed and loss is 1.6885535717010498\n",
            "Training Epoch0:  30% 115/388 [23:18<55:18, 12.16s/it]\n",
            " step 115 is completed and loss is 1.7131681442260742\n",
            "Training Epoch0:  30% 116/388 [23:31<55:06, 12.16s/it]\n",
            " step 116 is completed and loss is 1.6182128190994263\n",
            "Training Epoch0:  30% 117/388 [23:43<54:54, 12.16s/it]\n",
            " step 117 is completed and loss is 1.6003426313400269\n",
            "Training Epoch0:  30% 118/388 [23:55<54:41, 12.15s/it]\n",
            " step 118 is completed and loss is 1.664251446723938\n",
            "Training Epoch0:  31% 119/388 [24:07<54:29, 12.16s/it]\n",
            " step 119 is completed and loss is 1.5413126945495605\n",
            "Training Epoch0:  31% 120/388 [24:19<54:17, 12.16s/it]\n",
            " step 120 is completed and loss is 1.7349419593811035\n",
            "Training Epoch0:  31% 121/388 [24:31<54:04, 12.15s/it]\n",
            " step 121 is completed and loss is 1.6555522680282593\n",
            "Training Epoch0:  31% 122/388 [24:43<53:51, 12.15s/it]\n",
            " step 122 is completed and loss is 1.6149617433547974\n",
            "Training Epoch0:  32% 123/388 [24:56<53:42, 12.16s/it]\n",
            " step 123 is completed and loss is 1.6731948852539062\n",
            "Training Epoch0:  32% 124/388 [25:08<53:29, 12.16s/it]\n",
            " step 124 is completed and loss is 1.7450920343399048\n",
            "Training Epoch0:  32% 125/388 [25:20<53:17, 12.16s/it]\n",
            " step 125 is completed and loss is 1.6516002416610718\n",
            "Training Epoch0:  32% 126/388 [25:32<53:10, 12.18s/it]\n",
            " step 126 is completed and loss is 1.6808828115463257\n",
            "Training Epoch0:  33% 127/388 [25:44<52:56, 12.17s/it]\n",
            " step 127 is completed and loss is 1.710997223854065\n",
            "Training Epoch0:  33% 128/388 [25:57<52:52, 12.20s/it]\n",
            " step 128 is completed and loss is 1.676417350769043\n",
            "Training Epoch0:  33% 129/388 [26:09<52:35, 12.19s/it]\n",
            " step 129 is completed and loss is 1.6501035690307617\n",
            "Training Epoch0:  34% 130/388 [26:21<52:21, 12.18s/it]\n",
            " step 130 is completed and loss is 1.762292742729187\n",
            "Training Epoch0:  34% 131/388 [26:33<52:08, 12.17s/it]\n",
            " step 131 is completed and loss is 1.6793910264968872\n",
            "Training Epoch0:  34% 132/388 [26:45<51:54, 12.17s/it]\n",
            " step 132 is completed and loss is 1.641142725944519\n",
            "Training Epoch0:  34% 133/388 [26:58<51:48, 12.19s/it]\n",
            " step 133 is completed and loss is 1.7094802856445312\n",
            "Training Epoch0:  35% 134/388 [27:10<51:42, 12.21s/it]\n",
            " step 134 is completed and loss is 1.6217994689941406\n",
            "Training Epoch0:  35% 135/388 [27:22<51:28, 12.21s/it]\n",
            " step 135 is completed and loss is 1.6764416694641113\n",
            "Training Epoch0:  35% 136/388 [27:34<51:12, 12.19s/it]\n",
            " step 136 is completed and loss is 1.6656206846237183\n",
            "Training Epoch0:  35% 137/388 [27:46<50:56, 12.18s/it]\n",
            " step 137 is completed and loss is 1.6121180057525635\n",
            "Training Epoch0:  36% 138/388 [27:58<50:43, 12.17s/it]\n",
            " step 138 is completed and loss is 1.6201673746109009\n",
            "Training Epoch0:  36% 139/388 [28:11<50:30, 12.17s/it]\n",
            " step 139 is completed and loss is 1.6346616744995117\n",
            "Training Epoch0:  36% 140/388 [28:23<50:20, 12.18s/it]\n",
            " step 140 is completed and loss is 1.6792607307434082\n",
            "Training Epoch0:  36% 141/388 [28:35<50:06, 12.17s/it]\n",
            " step 141 is completed and loss is 1.5818709135055542\n",
            "Training Epoch0:  37% 142/388 [28:47<49:54, 12.17s/it]\n",
            " step 142 is completed and loss is 1.66032075881958\n",
            "Training Epoch0:  37% 143/388 [28:59<49:40, 12.17s/it]\n",
            " step 143 is completed and loss is 1.6259472370147705\n",
            "Training Epoch0:  37% 144/388 [29:12<49:32, 12.18s/it]\n",
            " step 144 is completed and loss is 1.6403898000717163\n",
            "Training Epoch0:  37% 145/388 [29:24<49:18, 12.18s/it]\n",
            " step 145 is completed and loss is 1.6658117771148682\n",
            "Training Epoch0:  38% 146/388 [29:36<49:05, 12.17s/it]\n",
            " step 146 is completed and loss is 1.6180930137634277\n",
            "Training Epoch0:  38% 147/388 [29:48<48:52, 12.17s/it]\n",
            " step 147 is completed and loss is 1.7081767320632935\n",
            "Training Epoch0:  38% 148/388 [30:00<48:39, 12.16s/it]\n",
            " step 148 is completed and loss is 1.7529672384262085\n",
            "Training Epoch0:  38% 149/388 [30:12<48:26, 12.16s/it]\n",
            " step 149 is completed and loss is 1.622977614402771\n",
            "Training Epoch0:  39% 150/388 [30:24<48:13, 12.16s/it]\n",
            " step 150 is completed and loss is 1.668562889099121\n",
            "Training Epoch0:  39% 151/388 [30:37<48:02, 12.16s/it]\n",
            " step 151 is completed and loss is 1.7437646389007568\n",
            "Training Epoch0:  39% 152/388 [30:49<47:49, 12.16s/it]\n",
            " step 152 is completed and loss is 1.6587580442428589\n",
            "Training Epoch0:  39% 153/388 [31:01<47:37, 12.16s/it]\n",
            " step 153 is completed and loss is 1.6659208536148071\n",
            "Training Epoch0:  40% 154/388 [31:13<47:29, 12.18s/it]\n",
            " step 154 is completed and loss is 1.6108007431030273\n",
            "Training Epoch0:  40% 155/388 [31:25<47:15, 12.17s/it]\n",
            " step 155 is completed and loss is 1.5529590845108032\n",
            "Training Epoch0:  40% 156/388 [31:37<47:03, 12.17s/it]\n",
            " step 156 is completed and loss is 1.5874172449111938\n",
            "Training Epoch0:  40% 157/388 [31:50<46:49, 12.16s/it]\n",
            " step 157 is completed and loss is 1.5839601755142212\n",
            "Training Epoch0:  41% 158/388 [32:02<46:37, 12.16s/it]\n",
            " step 158 is completed and loss is 1.6737737655639648\n",
            "Training Epoch0:  41% 159/388 [32:14<46:23, 12.16s/it]\n",
            " step 159 is completed and loss is 1.6503429412841797\n",
            "Training Epoch0:  41% 160/388 [32:26<46:12, 12.16s/it]\n",
            " step 160 is completed and loss is 1.6762057542800903\n",
            "Training Epoch0:  41% 161/388 [32:38<46:00, 12.16s/it]\n",
            " step 161 is completed and loss is 1.7201342582702637\n",
            "Training Epoch0:  42% 162/388 [32:51<45:55, 12.19s/it]\n",
            " step 162 is completed and loss is 1.631170630455017\n",
            "Training Epoch0:  42% 163/388 [33:03<45:39, 12.18s/it]\n",
            " step 163 is completed and loss is 1.6891745328903198\n",
            "Training Epoch0:  42% 164/388 [33:15<45:27, 12.17s/it]\n",
            " step 164 is completed and loss is 1.6234503984451294\n",
            "Training Epoch0:  43% 165/388 [33:27<45:13, 12.17s/it]\n",
            " step 165 is completed and loss is 1.6987459659576416\n",
            "Training Epoch0:  43% 166/388 [33:39<45:00, 12.16s/it]\n",
            " step 166 is completed and loss is 1.5186573266983032\n",
            "Training Epoch0:  43% 167/388 [33:51<44:52, 12.19s/it]\n",
            " step 167 is completed and loss is 1.6579856872558594\n",
            "Training Epoch0:  43% 168/388 [34:04<44:42, 12.20s/it]\n",
            " step 168 is completed and loss is 1.650329351425171\n",
            "Training Epoch0:  44% 169/388 [34:16<44:28, 12.18s/it]\n",
            " step 169 is completed and loss is 1.6160500049591064\n",
            "Training Epoch0:  44% 170/388 [34:28<44:14, 12.18s/it]\n",
            " step 170 is completed and loss is 1.6836096048355103\n",
            "Training Epoch0:  44% 171/388 [34:40<44:01, 12.17s/it]\n",
            " step 171 is completed and loss is 1.663450837135315\n",
            "Training Epoch0:  44% 172/388 [34:52<43:48, 12.17s/it]\n",
            " step 172 is completed and loss is 1.6293596029281616\n",
            "Training Epoch0:  45% 173/388 [35:04<43:37, 12.17s/it]\n",
            " step 173 is completed and loss is 1.745862364768982\n",
            "Training Epoch0:  45% 174/388 [35:17<43:24, 12.17s/it]\n",
            " step 174 is completed and loss is 1.5902034044265747\n",
            "Training Epoch0:  45% 175/388 [35:29<43:11, 12.17s/it]\n",
            " step 175 is completed and loss is 1.6536524295806885\n",
            "Training Epoch0:  45% 176/388 [35:41<42:58, 12.16s/it]\n",
            " step 176 is completed and loss is 1.5642536878585815\n",
            "Training Epoch0:  46% 177/388 [35:53<42:45, 12.16s/it]\n",
            " step 177 is completed and loss is 1.709384799003601\n",
            "Training Epoch0:  46% 178/388 [36:05<42:32, 12.15s/it]\n",
            " step 178 is completed and loss is 1.7032647132873535\n",
            "Training Epoch0:  46% 179/388 [36:17<42:20, 12.16s/it]\n",
            " step 179 is completed and loss is 1.6890722513198853\n",
            "Training Epoch0:  46% 180/388 [36:29<42:08, 12.15s/it]\n",
            " step 180 is completed and loss is 1.6815232038497925\n",
            "Training Epoch0:  47% 181/388 [36:42<41:55, 12.15s/it]\n",
            " step 181 is completed and loss is 1.6002732515335083\n",
            "Training Epoch0:  47% 182/388 [36:54<41:43, 12.15s/it]\n",
            " step 182 is completed and loss is 1.6386375427246094\n",
            "Training Epoch0:  47% 183/388 [37:06<41:31, 12.16s/it]\n",
            " step 183 is completed and loss is 1.7978278398513794\n",
            "Training Epoch0:  47% 184/388 [37:18<41:19, 12.16s/it]\n",
            " step 184 is completed and loss is 1.6632461547851562\n",
            "Training Epoch0:  48% 185/388 [37:30<41:13, 12.18s/it]\n",
            " step 185 is completed and loss is 1.7409305572509766\n",
            "Training Epoch0:  48% 186/388 [37:43<40:59, 12.18s/it]\n",
            " step 186 is completed and loss is 1.6705058813095093\n",
            "Training Epoch0:  48% 187/388 [37:55<40:45, 12.17s/it]\n",
            " step 187 is completed and loss is 1.708503007888794\n",
            "Training Epoch0:  48% 188/388 [38:07<40:33, 12.17s/it]\n",
            " step 188 is completed and loss is 1.6831680536270142\n",
            "Training Epoch0:  49% 189/388 [38:19<40:23, 12.18s/it]\n",
            " step 189 is completed and loss is 1.6680340766906738\n",
            "Training Epoch0:  49% 190/388 [38:31<40:07, 12.16s/it]\n",
            " step 190 is completed and loss is 1.6287211179733276\n",
            "Training Epoch0:  49% 191/388 [38:43<40:00, 12.18s/it]\n",
            " step 191 is completed and loss is 1.7396007776260376\n",
            "Training Epoch0:  49% 192/388 [38:56<39:50, 12.20s/it]\n",
            " step 192 is completed and loss is 1.5515167713165283\n",
            "Training Epoch0:  50% 193/388 [39:08<39:37, 12.19s/it]\n",
            " step 193 is completed and loss is 1.524642825126648\n",
            "Training Epoch0:  50% 194/388 [39:20<39:23, 12.18s/it]\n",
            " step 194 is completed and loss is 1.6184117794036865\n",
            "Training Epoch0:  50% 195/388 [39:32<39:09, 12.17s/it]\n",
            " step 195 is completed and loss is 1.6971447467803955\n",
            "Training Epoch0:  51% 196/388 [39:44<38:55, 12.16s/it]\n",
            " step 196 is completed and loss is 1.7237061262130737\n",
            "Training Epoch0:  51% 197/388 [39:56<38:42, 12.16s/it]\n",
            " step 197 is completed and loss is 1.6659445762634277\n",
            "Training Epoch0:  51% 198/388 [40:09<38:32, 12.17s/it]\n",
            " step 198 is completed and loss is 1.5592881441116333\n",
            "Training Epoch0:  51% 199/388 [40:21<38:18, 12.16s/it]\n",
            " step 199 is completed and loss is 1.6622340679168701\n",
            "Training Epoch0:  52% 200/388 [40:33<38:05, 12.16s/it]\n",
            " step 200 is completed and loss is 1.6296981573104858\n",
            "Training Epoch0:  52% 201/388 [40:45<37:53, 12.16s/it]\n",
            " step 201 is completed and loss is 1.6192221641540527\n",
            "Training Epoch0:  52% 202/388 [40:57<37:40, 12.15s/it]\n",
            " step 202 is completed and loss is 1.687212347984314\n",
            "Training Epoch0:  52% 203/388 [41:09<37:28, 12.16s/it]\n",
            " step 203 is completed and loss is 1.5905078649520874\n",
            "Training Epoch0:  53% 204/388 [41:21<37:16, 12.15s/it]\n",
            " step 204 is completed and loss is 1.6502599716186523\n",
            "Training Epoch0:  53% 205/388 [41:34<37:06, 12.16s/it]\n",
            " step 205 is completed and loss is 1.646639108657837\n",
            "Training Epoch0:  53% 206/388 [41:46<36:53, 12.16s/it]\n",
            " step 206 is completed and loss is 1.763948917388916\n",
            "Training Epoch0:  53% 207/388 [41:58<36:42, 12.17s/it]\n",
            " step 207 is completed and loss is 1.690037727355957\n",
            "Training Epoch0:  54% 208/388 [42:10<36:29, 12.16s/it]\n",
            " step 208 is completed and loss is 1.616627812385559\n",
            "Training Epoch0:  54% 209/388 [42:22<36:16, 12.16s/it]\n",
            " step 209 is completed and loss is 1.5812716484069824\n",
            "Training Epoch0:  54% 210/388 [42:35<36:05, 12.17s/it]\n",
            " step 210 is completed and loss is 1.616502285003662\n",
            "Training Epoch0:  54% 211/388 [42:47<35:57, 12.19s/it]\n",
            " step 211 is completed and loss is 1.7699717283248901\n",
            "Training Epoch0:  55% 212/388 [42:59<35:46, 12.20s/it]\n",
            " step 212 is completed and loss is 1.6156507730484009\n",
            "Training Epoch0:  55% 213/388 [43:11<35:33, 12.19s/it]\n",
            " step 213 is completed and loss is 1.6380358934402466\n",
            "Training Epoch0:  55% 214/388 [43:23<35:18, 12.18s/it]\n",
            " step 214 is completed and loss is 1.7347705364227295\n",
            "Training Epoch0:  55% 215/388 [43:35<35:06, 12.17s/it]\n",
            " step 215 is completed and loss is 1.6519513130187988\n",
            "Training Epoch0:  56% 216/388 [43:48<34:52, 12.17s/it]\n",
            " step 216 is completed and loss is 1.6885180473327637\n",
            "Training Epoch0:  56% 217/388 [44:00<34:39, 12.16s/it]\n",
            " step 217 is completed and loss is 1.6579548120498657\n",
            "Training Epoch0:  56% 218/388 [44:12<34:26, 12.16s/it]\n",
            " step 218 is completed and loss is 1.7069261074066162\n",
            "Training Epoch0:  56% 219/388 [44:24<34:14, 12.16s/it]\n",
            " step 219 is completed and loss is 1.6652451753616333\n",
            "Training Epoch0:  57% 220/388 [44:36<34:02, 12.16s/it]\n",
            " step 220 is completed and loss is 1.597661018371582\n",
            "Training Epoch0:  57% 221/388 [44:48<33:55, 12.19s/it]\n",
            " step 221 is completed and loss is 1.6693334579467773\n",
            "Training Epoch0:  57% 222/388 [45:01<33:43, 12.19s/it]\n",
            " step 222 is completed and loss is 1.696033239364624\n",
            "Training Epoch0:  57% 223/388 [45:13<33:33, 12.20s/it]\n",
            " step 223 is completed and loss is 1.6398558616638184\n",
            "Training Epoch0:  58% 224/388 [45:25<33:19, 12.19s/it]\n",
            " step 224 is completed and loss is 1.6410419940948486\n",
            "Training Epoch0:  58% 225/388 [45:37<33:05, 12.18s/it]\n",
            " step 225 is completed and loss is 1.5912929773330688\n",
            "Training Epoch0:  58% 226/388 [45:49<32:54, 12.19s/it]\n",
            " step 226 is completed and loss is 1.6963688135147095\n",
            "Training Epoch0:  59% 227/388 [46:02<32:41, 12.18s/it]\n",
            " step 227 is completed and loss is 1.6556799411773682\n",
            "Training Epoch0:  59% 228/388 [46:14<32:28, 12.18s/it]\n",
            " step 228 is completed and loss is 1.6754374504089355\n",
            "Training Epoch0:  59% 229/388 [46:26<32:15, 12.17s/it]\n",
            " step 229 is completed and loss is 1.6211764812469482\n",
            "Training Epoch0:  59% 230/388 [46:38<32:01, 12.16s/it]\n",
            " step 230 is completed and loss is 1.6255286931991577\n",
            "Training Epoch0:  60% 231/388 [46:50<31:49, 12.16s/it]\n",
            " step 231 is completed and loss is 1.8075933456420898\n",
            "Training Epoch0:  60% 232/388 [47:02<31:36, 12.16s/it]\n",
            " step 232 is completed and loss is 1.6617963314056396\n",
            "Training Epoch0:  60% 233/388 [47:15<31:25, 12.17s/it]\n",
            " step 233 is completed and loss is 1.7195810079574585\n",
            "Training Epoch0:  60% 234/388 [47:27<31:14, 12.17s/it]\n",
            " step 234 is completed and loss is 1.7208962440490723\n",
            "Training Epoch0:  61% 235/388 [47:39<31:01, 12.17s/it]\n",
            " step 235 is completed and loss is 1.704872488975525\n",
            "Training Epoch0:  61% 236/388 [47:51<30:48, 12.16s/it]\n",
            " step 236 is completed and loss is 1.659212350845337\n",
            "Training Epoch0:  61% 237/388 [48:03<30:38, 12.18s/it]\n",
            " step 237 is completed and loss is 1.530588150024414\n",
            "Training Epoch0:  61% 238/388 [48:15<30:25, 12.17s/it]\n",
            " step 238 is completed and loss is 1.615185022354126\n",
            "Training Epoch0:  62% 239/388 [48:28<30:12, 12.17s/it]\n",
            " step 239 is completed and loss is 1.6100387573242188\n",
            "Training Epoch0:  62% 240/388 [48:40<30:02, 12.18s/it]\n",
            " step 240 is completed and loss is 1.7215734720230103\n",
            "Training Epoch0:  62% 241/388 [48:52<29:53, 12.20s/it]\n",
            " step 241 is completed and loss is 1.6426196098327637\n",
            "Training Epoch0:  62% 242/388 [49:04<29:39, 12.19s/it]\n",
            " step 242 is completed and loss is 1.8234190940856934\n",
            "Training Epoch0:  63% 243/388 [49:16<29:26, 12.19s/it]\n",
            " step 243 is completed and loss is 1.6308249235153198\n",
            "Training Epoch0:  63% 244/388 [49:29<29:19, 12.22s/it]\n",
            " step 244 is completed and loss is 1.6747982501983643\n",
            "Training Epoch0:  63% 245/388 [49:41<29:04, 12.20s/it]\n",
            " step 245 is completed and loss is 1.7333327531814575\n",
            "Training Epoch0:  63% 246/388 [49:53<28:49, 12.18s/it]\n",
            " step 246 is completed and loss is 1.642595648765564\n",
            "Training Epoch0:  64% 247/388 [50:05<28:37, 12.18s/it]\n",
            " step 247 is completed and loss is 1.5591412782669067\n",
            "Training Epoch0:  64% 248/388 [50:17<28:24, 12.17s/it]\n",
            " step 248 is completed and loss is 1.7336437702178955\n",
            "Training Epoch0:  64% 249/388 [50:29<28:12, 12.17s/it]\n",
            " step 249 is completed and loss is 1.72152578830719\n",
            "Training Epoch0:  64% 250/388 [50:42<27:59, 12.17s/it]\n",
            " step 250 is completed and loss is 1.6403369903564453\n",
            "Training Epoch0:  65% 251/388 [50:54<27:46, 12.17s/it]\n",
            " step 251 is completed and loss is 1.717881441116333\n",
            "Training Epoch0:  65% 252/388 [51:06<27:34, 12.17s/it]\n",
            " step 252 is completed and loss is 1.6903128623962402\n",
            "Training Epoch0:  65% 253/388 [51:18<27:22, 12.16s/it]\n",
            " step 253 is completed and loss is 1.664702296257019\n",
            "Training Epoch0:  65% 254/388 [51:30<27:09, 12.16s/it]\n",
            " step 254 is completed and loss is 1.6901733875274658\n",
            "Training Epoch0:  66% 255/388 [51:42<26:57, 12.16s/it]\n",
            " step 255 is completed and loss is 1.704675316810608\n",
            "Training Epoch0:  66% 256/388 [51:55<26:45, 12.16s/it]\n",
            " step 256 is completed and loss is 1.6294645071029663\n",
            "Training Epoch0:  66% 257/388 [52:07<26:32, 12.16s/it]\n",
            " step 257 is completed and loss is 1.6044713258743286\n",
            "Training Epoch0:  66% 258/388 [52:19<26:20, 12.16s/it]\n",
            " step 258 is completed and loss is 1.6970953941345215\n",
            "Training Epoch0:  67% 259/388 [52:31<26:11, 12.18s/it]\n",
            " step 259 is completed and loss is 1.687727928161621\n",
            "Training Epoch0:  67% 260/388 [52:43<25:58, 12.18s/it]\n",
            " step 260 is completed and loss is 1.6316767930984497\n",
            "Training Epoch0:  67% 261/388 [52:55<25:47, 12.19s/it]\n",
            " step 261 is completed and loss is 1.7106999158859253\n",
            "Training Epoch0:  68% 262/388 [53:08<25:34, 12.18s/it]\n",
            " step 262 is completed and loss is 1.6678531169891357\n",
            "Training Epoch0:  68% 263/388 [53:20<25:21, 12.17s/it]\n",
            " step 263 is completed and loss is 1.5897387266159058\n",
            "Training Epoch0:  68% 264/388 [53:32<25:08, 12.17s/it]\n",
            " step 264 is completed and loss is 1.704365611076355\n",
            "Training Epoch0:  68% 265/388 [53:44<24:57, 12.17s/it]\n",
            " step 265 is completed and loss is 1.7080985307693481\n",
            "Training Epoch0:  69% 266/388 [53:56<24:44, 12.17s/it]\n",
            " step 266 is completed and loss is 1.6399928331375122\n",
            "Training Epoch0:  69% 267/388 [54:08<24:32, 12.17s/it]\n",
            " step 267 is completed and loss is 1.6606560945510864\n",
            "Training Epoch0:  69% 268/388 [54:21<24:19, 12.16s/it]\n",
            " step 268 is completed and loss is 1.6302253007888794\n",
            "Training Epoch0:  69% 269/388 [54:33<24:06, 12.16s/it]\n",
            " step 269 is completed and loss is 1.6667187213897705\n",
            "Training Epoch0:  70% 270/388 [54:45<23:54, 12.16s/it]\n",
            " step 270 is completed and loss is 1.6557427644729614\n",
            "Training Epoch0:  70% 271/388 [54:57<23:43, 12.16s/it]\n",
            " step 271 is completed and loss is 1.6709636449813843\n",
            "Training Epoch0:  70% 272/388 [55:09<23:31, 12.17s/it]\n",
            " step 272 is completed and loss is 1.6440776586532593\n",
            "Training Epoch0:  70% 273/388 [55:21<23:18, 12.16s/it]\n",
            " step 273 is completed and loss is 1.5977065563201904\n",
            "Training Epoch0:  71% 274/388 [55:34<23:06, 12.16s/it]\n",
            " step 274 is completed and loss is 1.580824375152588\n",
            "Training Epoch0:  71% 275/388 [55:46<22:53, 12.16s/it]\n",
            " step 275 is completed and loss is 1.6188336610794067\n",
            "Training Epoch0:  71% 276/388 [55:58<22:41, 12.16s/it]\n",
            " step 276 is completed and loss is 1.678653359413147\n",
            "Training Epoch0:  71% 277/388 [56:10<22:29, 12.15s/it]\n",
            " step 277 is completed and loss is 1.5955981016159058\n",
            "Training Epoch0:  72% 278/388 [56:22<22:20, 12.18s/it]\n",
            " step 278 is completed and loss is 1.6436524391174316\n",
            "Training Epoch0:  72% 279/388 [56:35<22:09, 12.20s/it]\n",
            " step 279 is completed and loss is 1.7108380794525146\n",
            "Training Epoch0:  72% 280/388 [56:47<21:55, 12.18s/it]\n",
            " step 280 is completed and loss is 1.5929465293884277\n",
            "Training Epoch0:  72% 281/388 [56:59<21:42, 12.17s/it]\n",
            " step 281 is completed and loss is 1.7372318506240845\n",
            "Training Epoch0:  73% 282/388 [57:11<21:30, 12.17s/it]\n",
            " step 282 is completed and loss is 1.6107555627822876\n",
            "Training Epoch0:  73% 283/388 [57:23<21:19, 12.19s/it]\n",
            " step 283 is completed and loss is 1.620473861694336\n",
            "Training Epoch0:  73% 284/388 [57:35<21:06, 12.18s/it]\n",
            " step 284 is completed and loss is 1.5824941396713257\n",
            "Training Epoch0:  73% 285/388 [57:48<20:54, 12.18s/it]\n",
            " step 285 is completed and loss is 1.7815680503845215\n",
            "Training Epoch0:  74% 286/388 [58:00<20:41, 12.17s/it]\n",
            " step 286 is completed and loss is 1.6473040580749512\n",
            "Training Epoch0:  74% 287/388 [58:12<20:28, 12.17s/it]\n",
            " step 287 is completed and loss is 1.6766998767852783\n",
            "Training Epoch0:  74% 288/388 [58:24<20:16, 12.16s/it]\n",
            " step 288 is completed and loss is 1.7045942544937134\n",
            "Training Epoch0:  74% 289/388 [58:36<20:03, 12.16s/it]\n",
            " step 289 is completed and loss is 1.7039028406143188\n",
            "Training Epoch0:  75% 290/388 [58:48<19:54, 12.18s/it]\n",
            " step 290 is completed and loss is 1.6513898372650146\n",
            "Training Epoch0:  75% 291/388 [59:01<19:42, 12.19s/it]\n",
            " step 291 is completed and loss is 1.7005739212036133\n",
            "Training Epoch0:  75% 292/388 [59:13<19:29, 12.18s/it]\n",
            " step 292 is completed and loss is 1.6534125804901123\n",
            "Training Epoch0:  76% 293/388 [59:25<19:17, 12.18s/it]\n",
            " step 293 is completed and loss is 1.6198022365570068\n",
            "Training Epoch0:  76% 294/388 [59:37<19:04, 12.18s/it]\n",
            " step 294 is completed and loss is 1.6441314220428467\n",
            "Training Epoch0:  76% 295/388 [59:49<18:51, 12.17s/it]\n",
            " step 295 is completed and loss is 1.6776542663574219\n",
            "Training Epoch0:  76% 296/388 [1:00:01<18:39, 12.17s/it]\n",
            " step 296 is completed and loss is 1.7731260061264038\n",
            "Training Epoch0:  77% 297/388 [1:00:14<18:27, 12.17s/it]\n",
            " step 297 is completed and loss is 1.7135053873062134\n",
            "Training Epoch0:  77% 298/388 [1:00:26<18:14, 12.16s/it]\n",
            " step 298 is completed and loss is 1.6523264646530151\n",
            "Training Epoch0:  77% 299/388 [1:00:38<18:02, 12.16s/it]\n",
            " step 299 is completed and loss is 1.6038073301315308\n",
            "Training Epoch0:  77% 300/388 [1:00:50<17:49, 12.16s/it]\n",
            " step 300 is completed and loss is 1.6666827201843262\n",
            "Training Epoch0:  78% 301/388 [1:01:02<17:41, 12.20s/it]\n",
            " step 301 is completed and loss is 1.6144919395446777\n",
            "Training Epoch0:  78% 302/388 [1:01:15<17:29, 12.20s/it]\n",
            " step 302 is completed and loss is 1.7469637393951416\n",
            "Training Epoch0:  78% 303/388 [1:01:27<17:15, 12.18s/it]\n",
            " step 303 is completed and loss is 1.6299316883087158\n",
            "Training Epoch0:  78% 304/388 [1:01:39<17:05, 12.20s/it]\n",
            " step 304 is completed and loss is 1.6949175596237183\n",
            "Training Epoch0:  79% 305/388 [1:01:51<16:51, 12.19s/it]\n",
            " step 305 is completed and loss is 1.6457244157791138\n",
            "Training Epoch0:  79% 306/388 [1:02:03<16:40, 12.20s/it]\n",
            " step 306 is completed and loss is 1.6192307472229004\n",
            "Training Epoch0:  79% 307/388 [1:02:15<16:26, 12.18s/it]\n",
            " step 307 is completed and loss is 1.654653787612915\n",
            "Training Epoch0:  79% 308/388 [1:02:28<16:14, 12.19s/it]\n",
            " step 308 is completed and loss is 1.6540300846099854\n",
            "Training Epoch0:  80% 309/388 [1:02:40<16:01, 12.17s/it]\n",
            " step 309 is completed and loss is 1.625375747680664\n",
            "Training Epoch0:  80% 310/388 [1:02:52<15:50, 12.18s/it]\n",
            " step 310 is completed and loss is 1.6123989820480347\n",
            "Training Epoch0:  80% 311/388 [1:03:04<15:38, 12.19s/it]\n",
            " step 311 is completed and loss is 1.707365870475769\n",
            "Training Epoch0:  80% 312/388 [1:03:16<15:25, 12.18s/it]\n",
            " step 312 is completed and loss is 1.8123513460159302\n",
            "Training Epoch0:  81% 313/388 [1:03:29<15:12, 12.17s/it]\n",
            " step 313 is completed and loss is 1.6892869472503662\n",
            "Training Epoch0:  81% 314/388 [1:03:41<15:00, 12.17s/it]\n",
            " step 314 is completed and loss is 1.6803779602050781\n",
            "Training Epoch0:  81% 315/388 [1:03:53<14:47, 12.16s/it]\n",
            " step 315 is completed and loss is 1.5850844383239746\n",
            "Training Epoch0:  81% 316/388 [1:04:05<14:35, 12.16s/it]\n",
            " step 316 is completed and loss is 1.674044132232666\n",
            "Training Epoch0:  82% 317/388 [1:04:17<14:24, 12.18s/it]\n",
            " step 317 is completed and loss is 1.5684444904327393\n",
            "Training Epoch0:  82% 318/388 [1:04:29<14:13, 12.20s/it]\n",
            " step 318 is completed and loss is 1.741471290588379\n",
            "Training Epoch0:  82% 319/388 [1:04:42<14:01, 12.20s/it]\n",
            " step 319 is completed and loss is 1.5884668827056885\n",
            "Training Epoch0:  82% 320/388 [1:04:54<13:49, 12.20s/it]\n",
            " step 320 is completed and loss is 1.6205286979675293\n",
            "Training Epoch0:  83% 321/388 [1:05:06<13:36, 12.19s/it]\n",
            " step 321 is completed and loss is 1.6795021295547485\n",
            "Training Epoch0:  83% 322/388 [1:05:18<13:23, 12.18s/it]\n",
            " step 322 is completed and loss is 1.7220590114593506\n",
            "Training Epoch0:  83% 323/388 [1:05:30<13:11, 12.17s/it]\n",
            " step 323 is completed and loss is 1.5931774377822876\n",
            "Training Epoch0:  84% 324/388 [1:05:43<12:59, 12.17s/it]\n",
            " step 324 is completed and loss is 1.6697887182235718\n",
            "Training Epoch0:  84% 325/388 [1:05:55<12:47, 12.18s/it]\n",
            " step 325 is completed and loss is 1.6715350151062012\n",
            "Training Epoch0:  84% 326/388 [1:06:07<12:34, 12.17s/it]\n",
            " step 326 is completed and loss is 1.7024282217025757\n",
            "Training Epoch0:  84% 327/388 [1:06:19<12:22, 12.17s/it]\n",
            " step 327 is completed and loss is 1.6464765071868896\n",
            "Training Epoch0:  85% 328/388 [1:06:31<12:09, 12.16s/it]\n",
            " step 328 is completed and loss is 1.5829763412475586\n",
            "Training Epoch0:  85% 329/388 [1:06:43<11:58, 12.18s/it]\n",
            " step 329 is completed and loss is 1.683292269706726\n",
            "Training Epoch0:  85% 330/388 [1:06:56<11:45, 12.17s/it]\n",
            " step 330 is completed and loss is 1.6102688312530518\n",
            "Training Epoch0:  85% 331/388 [1:07:08<11:35, 12.20s/it]\n",
            " step 331 is completed and loss is 1.6370458602905273\n",
            "Training Epoch0:  86% 332/388 [1:07:20<11:23, 12.21s/it]\n",
            " step 332 is completed and loss is 1.7035521268844604\n",
            "Training Epoch0:  86% 333/388 [1:07:32<11:10, 12.19s/it]\n",
            " step 333 is completed and loss is 1.6842228174209595\n",
            "Training Epoch0:  86% 334/388 [1:07:44<10:57, 12.18s/it]\n",
            " step 334 is completed and loss is 1.7044044733047485\n",
            "Training Epoch0:  86% 335/388 [1:07:56<10:44, 12.17s/it]\n",
            " step 335 is completed and loss is 1.664373517036438\n",
            "Training Epoch0:  87% 336/388 [1:08:09<10:32, 12.17s/it]\n",
            " step 336 is completed and loss is 1.6055101156234741\n",
            "Training Epoch0:  87% 337/388 [1:08:21<10:20, 12.16s/it]\n",
            " step 337 is completed and loss is 1.8337119817733765\n",
            "Training Epoch0:  87% 338/388 [1:08:33<10:08, 12.17s/it]\n",
            " step 338 is completed and loss is 1.6591473817825317\n",
            "Training Epoch0:  87% 339/388 [1:08:45<09:55, 12.16s/it]\n",
            " step 339 is completed and loss is 1.6268514394760132\n",
            "Training Epoch0:  88% 340/388 [1:08:57<09:43, 12.16s/it]\n",
            " step 340 is completed and loss is 1.6209824085235596\n",
            "Training Epoch0:  88% 341/388 [1:09:09<09:31, 12.16s/it]\n",
            " step 341 is completed and loss is 1.6467719078063965\n",
            "Training Epoch0:  88% 342/388 [1:09:22<09:19, 12.15s/it]\n",
            " step 342 is completed and loss is 1.7183992862701416\n",
            "Training Epoch0:  88% 343/388 [1:09:34<09:07, 12.17s/it]\n",
            " step 343 is completed and loss is 1.6925253868103027\n",
            "Training Epoch0:  89% 344/388 [1:09:46<08:56, 12.20s/it]\n",
            " step 344 is completed and loss is 1.6643911600112915\n",
            "Training Epoch0:  89% 345/388 [1:09:58<08:44, 12.19s/it]\n",
            " step 345 is completed and loss is 1.6081572771072388\n",
            "Training Epoch0:  89% 346/388 [1:10:10<08:31, 12.18s/it]\n",
            " step 346 is completed and loss is 1.738892912864685\n",
            "Training Epoch0:  89% 347/388 [1:10:23<08:19, 12.17s/it]\n",
            " step 347 is completed and loss is 1.7487014532089233\n",
            "Training Epoch0:  90% 348/388 [1:10:35<08:06, 12.17s/it]\n",
            " step 348 is completed and loss is 1.5247215032577515\n",
            "Training Epoch0:  90% 349/388 [1:10:47<07:54, 12.17s/it]\n",
            " step 349 is completed and loss is 1.7037588357925415\n",
            "Training Epoch0:  90% 350/388 [1:10:59<07:42, 12.16s/it]\n",
            " step 350 is completed and loss is 1.6197257041931152\n",
            "Training Epoch0:  90% 351/388 [1:11:11<07:29, 12.16s/it]\n",
            " step 351 is completed and loss is 1.672203540802002\n",
            "Training Epoch0:  91% 352/388 [1:11:23<07:17, 12.16s/it]\n",
            " step 352 is completed and loss is 1.6790424585342407\n",
            "Training Epoch0:  91% 353/388 [1:11:35<07:05, 12.15s/it]\n",
            " step 353 is completed and loss is 1.6475642919540405\n",
            "Training Epoch0:  91% 354/388 [1:11:48<06:53, 12.15s/it]\n",
            " step 354 is completed and loss is 1.6461979150772095\n",
            "Training Epoch0:  91% 355/388 [1:12:00<06:41, 12.15s/it]\n",
            " step 355 is completed and loss is 1.59192955493927\n",
            "Training Epoch0:  92% 356/388 [1:12:12<06:28, 12.15s/it]\n",
            " step 356 is completed and loss is 1.6557910442352295\n",
            "Training Epoch0:  92% 357/388 [1:12:24<06:16, 12.15s/it]\n",
            " step 357 is completed and loss is 1.65212082862854\n",
            "Training Epoch0:  92% 358/388 [1:12:36<06:04, 12.15s/it]\n",
            " step 358 is completed and loss is 1.6407356262207031\n",
            "Training Epoch0:  93% 359/388 [1:12:48<05:52, 12.15s/it]\n",
            " step 359 is completed and loss is 1.6869044303894043\n",
            "Training Epoch0:  93% 360/388 [1:13:01<05:40, 12.16s/it]\n",
            " step 360 is completed and loss is 1.683850884437561\n",
            "Training Epoch0:  93% 361/388 [1:13:13<05:28, 12.18s/it]\n",
            " step 361 is completed and loss is 1.5287063121795654\n",
            "Training Epoch0:  93% 362/388 [1:13:25<05:16, 12.17s/it]\n",
            " step 362 is completed and loss is 1.6582856178283691\n",
            "Training Epoch0:  94% 363/388 [1:13:37<05:04, 12.18s/it]\n",
            " step 363 is completed and loss is 1.6481839418411255\n",
            "Training Epoch0:  94% 364/388 [1:13:49<04:52, 12.17s/it]\n",
            " step 364 is completed and loss is 1.637181282043457\n",
            "Training Epoch0:  94% 365/388 [1:14:01<04:39, 12.17s/it]\n",
            " step 365 is completed and loss is 1.6704468727111816\n",
            "Training Epoch0:  94% 366/388 [1:14:14<04:28, 12.20s/it]\n",
            " step 366 is completed and loss is 1.6943657398223877\n",
            "Training Epoch0:  95% 367/388 [1:14:26<04:15, 12.18s/it]\n",
            " step 367 is completed and loss is 1.690016269683838\n",
            "Training Epoch0:  95% 368/388 [1:14:38<04:03, 12.18s/it]\n",
            " step 368 is completed and loss is 1.690165400505066\n",
            "Training Epoch0:  95% 369/388 [1:14:50<03:51, 12.17s/it]\n",
            " step 369 is completed and loss is 1.574110984802246\n",
            "Training Epoch0:  95% 370/388 [1:15:02<03:38, 12.16s/it]\n",
            " step 370 is completed and loss is 1.6866847276687622\n",
            "Training Epoch0:  96% 371/388 [1:15:14<03:26, 12.16s/it]\n",
            " step 371 is completed and loss is 1.656185507774353\n",
            "Training Epoch0:  96% 372/388 [1:15:27<03:14, 12.16s/it]\n",
            " step 372 is completed and loss is 1.65537428855896\n",
            "Training Epoch0:  96% 373/388 [1:15:39<03:02, 12.16s/it]\n",
            " step 373 is completed and loss is 1.653604507446289\n",
            "Training Epoch0:  96% 374/388 [1:15:51<02:50, 12.16s/it]\n",
            " step 374 is completed and loss is 1.6802581548690796\n",
            "Training Epoch0:  97% 375/388 [1:16:03<02:38, 12.16s/it]\n",
            " step 375 is completed and loss is 1.6949797868728638\n",
            "Training Epoch0:  97% 376/388 [1:16:15<02:25, 12.15s/it]\n",
            " step 376 is completed and loss is 1.5941370725631714\n",
            "Training Epoch0:  97% 377/388 [1:16:27<02:13, 12.15s/it]\n",
            " step 377 is completed and loss is 1.5748547315597534\n",
            "Training Epoch0:  97% 378/388 [1:16:40<02:01, 12.15s/it]\n",
            " step 378 is completed and loss is 1.6654940843582153\n",
            "Training Epoch0:  98% 379/388 [1:16:52<01:49, 12.15s/it]\n",
            " step 379 is completed and loss is 1.6984198093414307\n",
            "Training Epoch0:  98% 380/388 [1:17:04<01:37, 12.16s/it]\n",
            " step 380 is completed and loss is 1.654272198677063\n",
            "Training Epoch0:  98% 381/388 [1:17:16<01:25, 12.16s/it]\n",
            " step 381 is completed and loss is 1.7330271005630493\n",
            "Training Epoch0:  98% 382/388 [1:17:28<01:12, 12.16s/it]\n",
            " step 382 is completed and loss is 1.6259565353393555\n",
            "Training Epoch0:  99% 383/388 [1:17:40<01:00, 12.18s/it]\n",
            " step 383 is completed and loss is 1.6213757991790771\n",
            "Training Epoch0:  99% 384/388 [1:17:53<00:48, 12.17s/it]\n",
            " step 384 is completed and loss is 1.6577575206756592\n",
            "Training Epoch0:  99% 385/388 [1:18:05<00:36, 12.17s/it]\n",
            " step 385 is completed and loss is 1.7476844787597656\n",
            "Training Epoch0:  99% 386/388 [1:18:17<00:24, 12.16s/it]\n",
            " step 386 is completed and loss is 1.6264342069625854\n",
            "Training Epoch0: 100% 387/388 [1:18:29<00:12, 12.19s/it]\n",
            " step 387 is completed and loss is 1.7134017944335938\n",
            "Training Epoch0: 100% 388/388 [1:18:41<00:00, 12.17s/it]\n",
            "Max CUDA memory allocated was 21 GB\n",
            "Max CUDA memory reserved was 24 GB\n",
            "Peak active CUDA memory was 21 GB\n",
            "Cuda Malloc retires : 0\n",
            "CPU Total Peak Memory consumed during the train (max): 2 GB\n",
            "evaluating Epoch: 100% 84/84 [01:12<00:00,  1.16it/s]\n",
            " eval_ppl=tensor(5.2679, device='cuda:0') eval_epoch_loss=tensor(1.6616, device='cuda:0')\n",
            "we are about to save the PEFT modules\n",
            "PEFT modules are saved in /content/llama/PEFT/model directory\n",
            "best eval loss on epoch 0 is 1.66163170337677\n",
            "Epoch 1: train_perplexity=5.3936, train_epoch_loss=1.6852\n",
            "Training Epoch1:   0% 0/388 [00:00<?, ?it/s]\n",
            " step 0 is completed and loss is 1.6433852910995483\n",
            "Training Epoch1:   0% 1/388 [00:24<2:36:11, 24.22s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/llama-recipes/llama_finetuning.py\", line 237, in <module>\n",
            "    fire.Fire(main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/content/llama-recipes/llama_finetuning.py\", line 220, in main\n",
            "    results = train(\n",
            "  File \"/content/llama-recipes/utils/train_utils.py\", line 100, in train\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!export CUDA_VISIBLE_DEVICES=0\n",
        "!python llama_finetuning.py  --use_peft --peft_method lora --quantization --model_name /content/llama/models_hf/7B --output_dir /content/llama/PEFT/model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqqHmaGko3dK"
      },
      "outputs": [],
      "source": [
        "#对微调后的模型进行推理测试\n",
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from peft import PeftModel, PeftConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb7_bRq3pV6L"
      },
      "outputs": [],
      "source": [
        "model_id=\"/content/llama/models_hf/7B\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPM7aLU8p5KO"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(model, \"/content/llama/PEFT/model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SliF2KUFqkBK"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Summarize this dialog:\n",
        "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
        "B: I’m pretty sure I am. What’s up?\n",
        "A: Can you go with me to the animal shelter?.\n",
        "B: What do you want to do?\n",
        "A: I want to get a puppy for my son.\n",
        "B: That will make him so happy.\n",
        "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
        "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
        "A: I'll get him one of those little dogs.\n",
        "B: One that won't grow up too big;-)\n",
        "A: And eat too much;-))\n",
        "B: Do you know which one he would like?\n",
        "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
        "B: I bet you had to drag him away.\n",
        "A: He wanted to take it home right away ;-).\n",
        "B: I wonder what he'll name it.\n",
        "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
        "---\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#挂载google drive,保存微调模型\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AT1FVNKjX1S2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}